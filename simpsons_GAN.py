# -*- coding: utf-8 -*-
"""W4_shaheen_qq287550.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dlutiqOp1beEjPDbVfZ_1_rnzhz5KYOy
"""

import os
import time
import tensorflow as tf
import numpy as np
from glob import glob
import datetime
import random
from PIL import Image
import matplotlib.pyplot as plt
# %matplotlib inline

from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Activation
from tensorflow.keras.layers import LeakyReLU, BatchNormalization, Input
from tensorflow.keras.losses import categorical_crossentropy  
from tensorflow.keras.optimizers import Adam

"""### The following is the code used to import a dataset from kaggle --------"""

# import data using kaggle
!pip install -U -q kaggle
!mkdir -p ~/.kaggle

from google.colab import files
files.upload()

!cp kaggle.json ~/.kaggle/

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d kostastokis/simpsons-faces

# # Use this if only pulling dataset off of computer
# from google.colab import files
# files.upload()

!unzip /content/simpsons-faces.zip

# unzip files into folder
!unzip /content/cropped.zip -d /content/simpson_faces

"""### End import -----------"""

def create_generator():

  model = Sequential()

  # 4x4x512
  model.add(Dense(4*4*512,input_shape=(NOISE_SIZE,),name="dense1"))
  model.add(LeakyReLU(alpha=0.3))
  model.add(Reshape((4, 4, 512)))
  

  # 4x4x512 -> 8x8x512
  model.add(Conv2DTranspose(
      filters=512,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="trans_conv1",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_trans_conv1",
  ))

  model.add(LeakyReLU(alpha=0.3, name="trans_conv1_out"))

  # 8x8x512 -> 16x16x256
  model.add(Conv2DTranspose(
      filters=256,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="trans_conv2",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_trans_conv2",
  ))

  model.add(LeakyReLU(alpha=0.3, name="trans_conv2_out"))            

  # 16x16x256 -> 32x32x128
  model.add(Conv2DTranspose(
      filters=128,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="trans_con3",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_trans_conv3",
  ))

  model.add(LeakyReLU(alpha=0.3, name="trans_conv3_out"))     

  # 32x32x128 -> 64x64x64
  model.add(Conv2DTranspose(
      filters=64,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="trans_con4",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_trans_conv4",
  ))

  model.add(LeakyReLU(alpha=0.3, name="trans_conv4_out"))          

  # 64x64x64 -> 64x64x3
  model.add(Conv2DTranspose(
      filters=3,
      kernel_size=[5,5],
      strides=[1,1],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="logits",
  ))

  model.add(Activation('tanh'))
  
  model.summary()

  return model

def create_discriminator():

  model = Sequential()
  
  # 64x64x3 -> 32x32x32 
  model.add(Conv2D(
      filters=32,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="conv1",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_norm1",
  ))

  model.add(LeakyReLU(alpha=0.3, name="conv1_out"))
  
  # 32x32x32 -> 16x16x64 
  model.add(Conv2D(
      filters=64,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="conv2",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_norm2",
  ))

  model.add(LeakyReLU(alpha=0.3, name="conv2_out"))  
  
  # 16x16x64 -> 8x8x128  
  model.add(Conv2D(
      filters=128,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="conv3",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_norm3",
  ))

  model.add(LeakyReLU(alpha=0.3, name="conv3_out"))  
  
  # 8x8x128 -> 8x8x256
  model.add(Conv2D(
      filters=256,
      kernel_size=[5,5],
      strides=[1,1],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="conv4",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_norm4",
  ))

  model.add(LeakyReLU(alpha=0.3, name="conv4_out"))  

  # 8x8x256 -> 4x4x512
  model.add(Conv2D(
      filters=512,
      kernel_size=[5,5],
      strides=[2,2],
      padding="SAME",
      kernel_initializer=tf.keras.initializers.TruncatedNormal(
          stddev=WEIGHT_INIT_STDDEV),
      name="conv5",
  ))

  model.add(BatchNormalization(
      epsilon=EPSILON,
      name="batch_norm5",
  ))

  model.add(LeakyReLU(alpha=0.3, name="conv5_out"))  
  
  model.add(Flatten())

  model.add(Dense(1, activation='sigmoid'))
    
  model.summary()
  
  return model

def create_GAN(generator, discriminator):  
  gan_input = (Input(shape=(NOISE_SIZE,)))  
  x = generator(gan_input)  
  gan_output = discriminator(x)  
  model = Model(inputs=gan_input, outputs=gan_output)
  
  return model

def show_samples(sample_images, name, epoch):
  figure, axes = plt.subplots(
      1, 
      len(sample_images), 
      figsize = (IMAGE_SIZE, IMAGE_SIZE),
  )
  for index, axis in enumerate(axes):
      axis.axis('off')
      image_array = sample_images[index]
      axis.imshow(image_array)
      image = Image.fromarray(image_array)
      image.save(name+"_"+str(epoch)+"_"+str(index)+".png") 
  plt.savefig(name+"_"+str(epoch)+".png", bbox_inches='tight', pad_inches=0)
  plt.show()
  plt.close()

def test(generator, out_channel_dim, epoch):
  example_z = np.random.uniform(-1, 1, size=(SAMPLES_TO_SHOW, NOISE_SIZE))
  samples = generator.predict(example_z)
  sample_images = [((sample + 1.0) * 127.5).astype(np.uint8)
                   for sample in samples]
  show_samples(sample_images, OUTPUT_DIR + "samples", epoch)

def summarize_epoch(epoch, duration, d_losses,
                    g_losses, generator, data_shape):
  minibatch_size = int(data_shape[0]//BATCH_SIZE)
  print("Epoch {}/{}".format(epoch, EPOCHS),
        "\nDuration: {:.5f}".format(duration),
        "\nD Loss: {:.5f}".format(np.mean(d_losses[-minibatch_size:])),
        "\nG Loss: {:.5f}".format(np.mean(g_losses[-minibatch_size:])))
  fig, ax = plt.subplots()
  plt.plot(d_losses, label='Discriminator', alpha=0.6)
  plt.plot(g_losses, label='Generator', alpha=0.6)
  plt.title("Losses")
  plt.legend()
  plt.savefig(OUTPUT_DIR + "losses_" + str(epoch) + ".png")
  plt.show()
  plt.close()
  test(generator, data_shape[3], epoch)

# Split the input images into batches of BATCH_SIZE
# and augment random images
def get_batches(data):
  np.random.shuffle(data)
  batches = []
  for i in range(int(data.shape[0]//BATCH_SIZE)):
      batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]
      augmented_images = []
      for img in batch:
          image = Image.fromarray(img)
          if random.choice([True, False]):
              image = image.transpose(Image.FLIP_LEFT_RIGHT)
          augmented_images.append(np.asarray(image))
      batch = np.asarray(augmented_images)
      normalized_batch = (batch / 127.5) - 1.0
      batches.append(normalized_batch)
  return batches

def train(input_images, data_shape, checkpoint_to_load=None):  
  generator = create_generator()
   
  discriminator = create_discriminator()
  discriminator.compile(loss='binary_crossentropy', optimizer=
              Adam(lr=LR_D, beta_1=BETA1))
  
  gan = create_GAN(generator, discriminator)
  discriminator.trainable = False
  gan.compile(loss='binary_crossentropy', optimizer=
              Adam(lr=LR_G, beta_1=BETA1))
  
  if checkpoint_to_load != None:
    discriminator.load_weights(checkpoint_to_load[0])
    gan.load_weights(checkpoint_to_load[1])
 
  d_losses = []
  g_losses = []
  
  for epoch in range(EPOCHS):    
    batches = get_batches(input_images)
    epoch += 1
    start_time = time.time() 
    print('-'*20, 'Epoch ', epoch, '-'*20)
    
    for i in range(int(data_shape[0]//BATCH_SIZE)):
      # Train discriminator
      real_image_set = batches[i]
      deck = np.arange(0, real_image_set.shape[0])
      random.shuffle(deck)
      
      real_images = real_image_set[deck[0:int(BATCH_SIZE/2)],:,:,:]
      gauss_noise = np.random.normal(
          0.0,random.uniform(0.0, 0.1),(real_images.shape))
      gauss_noise = gauss_noise.reshape(real_images.shape)      
      noisy_real_images = real_images + gauss_noise      

      batch_z = np.random.uniform(-1, 1, size=(int(BATCH_SIZE/2), NOISE_SIZE))    
      fake_images = generator.predict(batch_z)
      
#       # Train on soft targets (add noise to targets as well)
#       noise_prop = 0.05 # Randomly flip 5% of targets
      
#       true_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(
#           low=0.0, high=0.1, size=(BATCH_SIZE, 1))
#       flipped_idx = np.random.choice(np.arange(len(true_labels)),
#                                      size=int(noise_prop*len(true_labels)))
#       true_labels[flipped_idx] = 1 - true_labels[flipped_idx]
      
#       gene_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(
#           low=0.0, high=0.1, size=(BATCH_SIZE, 1))
#       flipped_idx = np.random.choice(np.arange(len(gene_labels)), 
#                                      size=int(noise_prop*len(gene_labels)))
#       gene_labels[flipped_idx] = 1 - gene_labels[flipped_idx]
      
      true_labels = np.ones((int(BATCH_SIZE/2), 1))*random.uniform(0.9, 1.0)
      gene_labels = np.zeros((int(BATCH_SIZE/2), 1))

#       x_combined_batch = np.concatenate((noisy_real_images, fake_images))
#       y_combined_batch = np.concatenate((
#           true_labels,
#           gene_labels, 
#           ))
      
#       d_loss = discriminator.train_on_batch(x_combined_batch, y_combined_batch)

      d_loss_real = discriminator.train_on_batch(
          noisy_real_images,true_labels)  
      d_loss_fake = discriminator.train_on_batch(
          fake_images,gene_labels)
      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
      
      # Train generator
      batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))    
      y_mislabeled = np.concatenate(np.ones((BATCH_SIZE, 1)))

      g_loss = gan.train_on_batch(batch_z, y_mislabeled)
      
      # Append losses
      d_losses.append(d_loss)
      g_losses.append(g_loss)
    
    if epoch > 0 and epoch % 20 == 0:
      print("saving model")
      discriminator.save_weights("desc-simposon-model.h5-" + str(epoch))
      gan.save_weights("gan-simposon-model.h5-" + str(epoch))

    summarize_epoch(epoch, time.time()-start_time, d_losses, 
                    g_losses, generator, data_shape)

# Paths
# Path to the folder with input images
INPUT_DATA_DIR = "/content/simpson_faces/" 
OUTPUT_DIR = './{date:%Y-%m-%d_%H:%M:%S}/'.format(date=datetime.datetime.now())
if not os.path.exists(OUTPUT_DIR):
  os.makedirs(OUTPUT_DIR)

# Hyperparameters
IMAGE_SIZE = 64 # Originally 128
NOISE_SIZE = 100
LR_D = 0.00004
LR_G = 0.0004
BATCH_SIZE = 64
EPOCHS = 500
BETA1 = 0.5
WEIGHT_INIT_STDDEV = 0.02
EPSILON = 0.00005
SAMPLES_TO_SHOW = 5

exclusion_list = [
    "9746","9731","9717","9684","9637","9641","9642","9584","9541","9535",
    "9250","9251","9252","9043","8593","8584","8052","8051","8008","7957",
    "7958","7761","7762","9510","9307","4848","4791","4785","4465","2709",
    "7724","7715","7309","7064","7011","6961","6962","6963","6960","6949",
    "6662","6496","6409","6411","6406","6407","6170","6171","6172","5617",
    "4363","4232","4086","4047","3894","3889","3493","3393","3362","2780",
    "2710","2707","2708","2711","2712","2309","2056","1943","1760","1743",
    "1702","1281","1272","772","736","737","691","684","314","242","191",
]

# Training

# Remove the unwanted files from the exclusion list
for f in exclusion_list:
    # or depending on situation: f.rstrip('\n')
    # or, if you get rid of os.chdir(path) above,
    fname = INPUT_DATA_DIR + f +'.png'
    if os.path.isfile(fname): # this makes the code more robust
      os.remove(fname)
      
# Import and rescale images
input_images = np.asarray([
    np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE)))
    for file in glob(INPUT_DATA_DIR + '*.png')
])
print ("Input: " + str(input_images.shape))

np.random.shuffle(input_images)

sample_images = random.sample(list(input_images), SAMPLES_TO_SHOW)
show_samples(sample_images, OUTPUT_DIR + "inputs", 0)

train(input_images, input_images.shape)

